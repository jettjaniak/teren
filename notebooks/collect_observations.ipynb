{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# needed for set_determinism\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:16:8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from datasets import Dataset, load_dataset\n",
    "from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "from typing import Optional, Tuple\n",
    "from jaxtyping import Float\n",
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from random import choice, shuffle\n",
    "from typing import final, cast\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_device_str() -> str:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Utils\n",
    "def generate_prompt(dataset, n_ctx: int = 1, batch: int = 1) -> torch.Tensor:\n",
    "    \"\"\"Generate a prompt from the dataset.\"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch, shuffle=True)\n",
    "    return next(iter(dataloader))[\"input_ids\"][:, :n_ctx]\n",
    "\n",
    "\n",
    "def compute_kl_div(logits_ref: torch.Tensor, logits_pert: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the KL divergence between the reference and perturbed logprobs.\"\"\"\n",
    "    logprobs_ref = F.log_softmax(logits_ref, dim=-1)\n",
    "    logprobs_pert = F.log_softmax(logits_pert, dim=-1)\n",
    "    temp_output = F.kl_div(\n",
    "        logprobs_ref, logprobs_pert, log_target=True, reduction=\"none\"\n",
    "    )\n",
    "    return temp_output.sum(dim=-1)\n",
    "\n",
    "\n",
    "def get_random_activation(\n",
    "    model: HookedTransformer, dataset: Dataset, n_ctx: int, layer: str, pos\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get a random activation from the dataset.\"\"\"\n",
    "    rand_prompt = generate_prompt(dataset, n_ctx=n_ctx)\n",
    "    _, cache = model.run_with_cache(rand_prompt)\n",
    "    return cache[layer][:, pos, :].to(\"cpu\").detach()\n",
    "\n",
    "\n",
    "def load_pretokenized_dataset(\n",
    "    path: str,\n",
    "    split: str,\n",
    ") -> Dataset:\n",
    "    dataset = load_dataset(path, split=split)\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    return dataset.with_format(\"torch\")\n",
    "\n",
    "\n",
    "def get_random_activations(\n",
    "    model: HookedTransformer, dataset: Dataset, n_ctx: int, layer: str, pos, n_samples\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get a random activation from the dataset.\"\"\"\n",
    "    rand_prompts = torch.cat(\n",
    "        [generate_prompt(dataset, n_ctx=n_ctx) for _ in range(n_samples)]\n",
    "    )\n",
    "    _, cache = model.run_with_cache(rand_prompts)\n",
    "    return cache[layer][:, pos, :].to(\"cpu\").detach()\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    n_ctx: int\n",
    "    perturbation_layer: str\n",
    "    read_layer: str\n",
    "    perturbation_pos: slice\n",
    "    n_steps: int\n",
    "    perturbation_range: Tuple[float, float]\n",
    "    seed: Optional[int] = None\n",
    "    dataloader_batch_size: Optional[int] = None\n",
    "    mean_batch_size: Optional[int] = None\n",
    "\n",
    "\n",
    "class Reference:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: HookedTransformer,\n",
    "        prompt: torch.Tensor,\n",
    "        perturbation_layer: str,\n",
    "        read_layer: str,\n",
    "        perturbation_pos: slice,\n",
    "        n_ctx: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        _, n_ctx_prompt = prompt.shape\n",
    "        assert (\n",
    "            n_ctx == n_ctx_prompt\n",
    "        ), f\"n_ctx {n_ctx} must match prompt n_ctx {n_ctx_prompt}\"\n",
    "        self.prompt = prompt\n",
    "        logits, cache = model.run_with_cache(prompt)\n",
    "        self.logits = logits.to(\"cpu\").detach()\n",
    "        self.cache = cache.to(\"cpu\")\n",
    "        self.act = self.cache[perturbation_layer][:, perturbation_pos]\n",
    "        self.perturbation_layer = perturbation_layer\n",
    "        self.read_layer = read_layer\n",
    "        self.perturbation_pos = perturbation_pos\n",
    "        self.n_ctx = n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ExperimentConfig(\n",
    "    n_ctx=10,\n",
    "    perturbation_layer=\"blocks.0.hook_resid_pre\",\n",
    "    seed=9944,\n",
    "    dataloader_batch_size=15,\n",
    "    perturbation_pos=slice(-1, None, 1),\n",
    "    read_layer=\"blocks.11.hook_resid_post\",\n",
    "    perturbation_range=(0, 1),\n",
    "    n_steps=100,\n",
    "    mean_batch_size=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb94226501041d5a09e52dca064a715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9636d41e6aa544759cb5cecd298266d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e3c2fca6c14598a2417f61e54b4e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_pretokenized_dataset(\n",
    "    path=\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\"\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=cfg.dataloader_batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2\")\n",
    "device = get_device_str()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SAE(\n",
       "  (activation_fn): ReLU()\n",
       "  (hook_sae_input): HookPoint()\n",
       "  (hook_sae_acts_pre): HookPoint()\n",
       "  (hook_sae_acts_post): HookPoint()\n",
       "  (hook_sae_output): HookPoint()\n",
       "  (hook_sae_recons): HookPoint()\n",
       "  (hook_sae_error): HookPoint()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes, sparsities = get_gpt2_res_jb_saes(cfg.perturbation_layer)\n",
    "sae = saes[cfg.perturbation_layer].cpu()\n",
    "feature_sparsities = 10 ** sparsities[cfg.perturbation_layer].cpu()\n",
    "\n",
    "sae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from uuid import uuid4\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "from jaxtyping import Float, Int\n",
    "import os\n",
    "from datetime import datetime\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id():\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "def get_k_random_prompts(dataset, n_ctx=10, k=100):\n",
    "    n_total = len(dataset)\n",
    "    idxs = random.sample(range(n_total), k)\n",
    "    input_ids = dataset[idxs][\"input_ids\"][:, :n_ctx]\n",
    "    return input_ids.to(\"cpu\")  # Move to CPU immediately\n",
    "\n",
    "\n",
    "def collect_info_act(cfg, act, sae, feature_sparsities):\n",
    "    with torch.no_grad():\n",
    "        all_feature_acts = sae.encode(act.cpu().unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    active_mask = all_feature_acts > 0.0\n",
    "    active_feature_ids = active_mask.nonzero().squeeze().tolist()\n",
    "    active_feature_acts = all_feature_acts[active_mask].tolist()\n",
    "    active_feature_sparsities = feature_sparsities[active_mask].to(device).tolist()\n",
    "    num_active_features = len(active_feature_ids)\n",
    "\n",
    "    return {\n",
    "        \"active_feature_ids\": active_feature_ids,\n",
    "        \"active_feature_acts\": active_feature_acts,\n",
    "        \"active_feature_sparsities\": active_feature_sparsities,\n",
    "        \"num_active_features\": num_active_features,\n",
    "    }\n",
    "\n",
    "\n",
    "def perturb_activation(start_act, end_act, num_steps=100):\n",
    "    device = start_act.device\n",
    "    t = torch.linspace(0, 1, num_steps, device=device).unsqueeze(1)\n",
    "    return start_act * (1 - t) + end_act * t\n",
    "\n",
    "\n",
    "def run_with_perturbation(cfg, model, prompt, perturbed_acts):\n",
    "    def hook(act, hook):\n",
    "        act[:, -1, :] = perturbed_acts\n",
    "\n",
    "    prompts = prompt.repeat(perturbed_acts.shape[0], 1)\n",
    "\n",
    "    with model.hooks(fwd_hooks=[(cfg.perturbation_layer, hook)]):\n",
    "        logits_pert, cache = model.run_with_cache(prompts)\n",
    "\n",
    "    return logits_pert, cache\n",
    "\n",
    "\n",
    "def comp_js_divergence(\n",
    "    p_logit: Float[torch.Tensor, \"*batch vocab\"],\n",
    "    q_logit: Float[torch.Tensor, \"*batch vocab\"],\n",
    ") -> Float[torch.Tensor, \"*batch\"]:\n",
    "    p_logprob = torch.log_softmax(p_logit, dim=-1)\n",
    "    q_logprob = torch.log_softmax(q_logit, dim=-1)\n",
    "    p = p_logprob.exp()\n",
    "    q = q_logprob.exp()\n",
    "\n",
    "    # convert to log2\n",
    "    p_logprob *= math.log2(math.e)\n",
    "    q_logprob *= math.log2(math.e)\n",
    "\n",
    "    m = 0.5 * (p + q)\n",
    "    m_logprob = m.log2()\n",
    "\n",
    "    p_kl_div = (p * (p_logprob - m_logprob)).sum(-1)\n",
    "    q_kl_div = (q * (q_logprob - m_logprob)).sum(-1)\n",
    "\n",
    "    assert p_kl_div.isfinite().all()\n",
    "    assert q_kl_div.isfinite().all()\n",
    "    return (p_kl_div + q_kl_div) / 2\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_observation(\n",
    "    cfg,\n",
    "    model,\n",
    "    start_act,\n",
    "    end_act,\n",
    "    start_prompt,\n",
    "    start_logits,\n",
    "    start_logprobs,\n",
    "    sae,\n",
    "    feature_sparsities,\n",
    "):\n",
    "    device = start_act.device\n",
    "    perturbed_acts = perturb_activation(start_act, end_act, cfg.n_steps).to(device)\n",
    "\n",
    "    # Verify that we start at source and end at target\n",
    "    assert torch.allclose(\n",
    "        perturbed_acts[0], start_act, atol=1e-5\n",
    "    ), \"Doesn't start at source\"\n",
    "    assert torch.allclose(\n",
    "        perturbed_acts[-1], end_act, atol=1e-5\n",
    "    ), \"Doesn't end at target\"\n",
    "\n",
    "    pert_logits, cache = run_with_perturbation(cfg, model, start_prompt, perturbed_acts)\n",
    "\n",
    "    read_layer_l2_norms = torch.norm(\n",
    "        cache[cfg.read_layer][:, -1, :] - cache[cfg.read_layer][0, -1, :],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    pert_logprobs = F.log_softmax(pert_logits[:, -1, :], dim=-1)\n",
    "    kl_divs = F.kl_div(\n",
    "        pert_logprobs, start_logprobs, log_target=True, reduction=\"none\"\n",
    "    ).sum(dim=-1)\n",
    "    # kl_divs = (pert_logprobs.exp() * (pert_logprobs - start_logprobs)).sum(dim=-1)\n",
    "\n",
    "    js_divs = comp_js_divergence(pert_logits[:, -1, :], start_logits[:, -1, :])\n",
    "    js_dist = torch.sqrt(js_divs + 1e-8)\n",
    "\n",
    "    perturbation_steps_metadata = []\n",
    "\n",
    "    for step, pert_act in enumerate(perturbed_acts):\n",
    "        cos_sim = F.cosine_similarity(\n",
    "            pert_act.unsqueeze(0), start_act.unsqueeze(0)\n",
    "        ).item()\n",
    "        l2_norm = torch.norm(pert_act - start_act).item()\n",
    "        act_info = collect_info_act(cfg, pert_act, sae, feature_sparsities)\n",
    "        perturbation_steps_metadata.append(\n",
    "            {\n",
    "                \"step\": step + 1,\n",
    "                \"kl_div\": kl_divs[step].item(),\n",
    "                \"js_div\": js_divs[step].item(),\n",
    "                \"js_dist\": js_dist[step].item(),\n",
    "                \"read_layer_l2_norm\": read_layer_l2_norms[step].item(),\n",
    "                \"cos_sim\": cos_sim,\n",
    "                \"l2_norm\": l2_norm,\n",
    "                **act_info,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    del pert_logits, pert_logprobs, kl_divs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return perturbation_steps_metadata, perturbed_acts.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_observations(\n",
    "    cfg, model, dataset, sae, feature_sparsities, k=10, trials_per_prompt=10\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "    start_prompts = get_k_random_prompts(dataset, k=k)\n",
    "\n",
    "    for start_prompt in start_prompts:\n",
    "        with torch.no_grad():\n",
    "            start_prompt_gpu = start_prompt.to(device)\n",
    "            start_logits, cache = model.run_with_cache(start_prompt_gpu)\n",
    "            start_logprobs = F.log_softmax(start_logits[:, -1, :], dim=-1)\n",
    "            start_act = cache[cfg.perturbation_layer][:, -1, :].squeeze(0)\n",
    "\n",
    "        end_prompts = get_k_random_prompts(dataset, k=trials_per_prompt)\n",
    "\n",
    "        for end_prompt in end_prompts:\n",
    "            with torch.no_grad():\n",
    "                end_prompt_gpu = end_prompt.to(device)\n",
    "                _, cache = model.run_with_cache(end_prompt_gpu)\n",
    "                end_act = cache[cfg.perturbation_layer][:, -1, :].squeeze(0)\n",
    "\n",
    "            steps_metadata, perturbed_acts = collect_observation(\n",
    "                cfg,\n",
    "                model,\n",
    "                start_act,\n",
    "                end_act,\n",
    "                start_prompt_gpu,\n",
    "                start_logits,\n",
    "                start_logprobs,\n",
    "                sae,\n",
    "                feature_sparsities,\n",
    "            )\n",
    "\n",
    "            observation = {\n",
    "                \"id\": generate_id(),\n",
    "                \"start_prompt\": start_prompt.tolist(),\n",
    "                \"end_prompt\": end_prompt.tolist(),\n",
    "                \"steps_metadata\": steps_metadata,\n",
    "                \"perturbed_acts\": perturbed_acts,\n",
    "            }\n",
    "            yield observation\n",
    "\n",
    "        del (\n",
    "            start_logits,\n",
    "            cache,\n",
    "            start_logprobs,\n",
    "            start_act,\n",
    "            end_prompts,\n",
    "            start_prompt_gpu,\n",
    "        )\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_float_precision(obj, precision=4):\n",
    "    if isinstance(obj, float):\n",
    "        return float(np.round(obj, precision))\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: reduce_float_precision(v, precision) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [reduce_float_precision(i, precision) for i in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_observations_to_disk(\n",
    "    observations, activations, base_dir=\"observations\", precision=4\n",
    "):\n",
    "    if not observations:\n",
    "        return\n",
    "\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"metadata_{len(observations)}_{timestamp}.jsonl\"\n",
    "    acts_filename = f\"acts_{len(observations)}_{timestamp}.npy\"\n",
    "    filepath = os.path.join(base_dir, filename)\n",
    "    acts_filepath = os.path.join(base_dir, acts_filename)\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for obs in observations:\n",
    "            reduced_obs = reduce_float_precision(obs, precision)\n",
    "            json.dump(reduced_obs, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    np.save(acts_filepath, np.stack(activations))\n",
    "\n",
    "    print(f\"Saved {len(observations)} observations & acts to {filepath}/npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_counter = 0\n",
    "max_observations_per_file = 1000\n",
    "\n",
    "metadata, activations = [], []\n",
    "\n",
    "for observation in get_observations(\n",
    "    cfg, model, dataset, sae, feature_sparsities, k=30_000, trials_per_prompt=1\n",
    "):\n",
    "    metadata.append(\n",
    "        {\n",
    "            \"id\": observation[\"id\"],\n",
    "            \"start_prompt\": observation[\"start_prompt\"],\n",
    "            \"end_prompt\": observation[\"end_prompt\"],\n",
    "            \"steps_metadata\": observation[\"steps_metadata\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    activations.append(observation[\"perturbed_acts\"])\n",
    "\n",
    "    # Save to disk and clear when we reach the max number of observations per file\n",
    "    if len(metadata) >= max_observations_per_file:\n",
    "        save_observations_to_disk(metadata, activations)\n",
    "        metadata = []\n",
    "        activations = []\n",
    "        file_counter += 1\n",
    "\n",
    "# Save any remaining observations\n",
    "if metadata:\n",
    "    save_observations_to_disk(metadata, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
