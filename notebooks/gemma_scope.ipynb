{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# needed for set_determinism\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:16:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/paperspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import plotly.express as px\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import cast\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def load_pretokenized_dataset(\n",
    "    path: str,\n",
    "    split: str,\n",
    ") -> Dataset:\n",
    "    dataset = load_dataset(path, split=split)\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    return dataset.with_format(\"torch\")\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_device_str() -> str:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def download_sae_feature_explanations():\n",
    "\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    # payload = {\n",
    "    #     \"modelId\": \"gpt2-small\",\n",
    "    #     \"saeId\": \"7-res-jb\",\n",
    "    # }\n",
    "    payload = {\"modelId\": \"gemma-2-2b\", \"saeId\": \"20-gemmascope-res-16k\"}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    # convert to pandas\n",
    "    explanations_df = pd.DataFrame(response.json()[\"explanations\"])\n",
    "    # rename index to \"feature\"\n",
    "    explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "    # explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "    explanations_df[\"description\"] = explanations_df[\"description\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "    return explanations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(2462462)\n",
    "\n",
    "device = get_device_str()\n",
    "print(device)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b02540155894564b5f43ac385878421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bebf7a3b914ecfadec6882563e10e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b49cc16f42a4cd8a9f4d5a9ce9cf472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_pretokenized_dataset(\n",
    "    path=\"apollo-research/Skylion007-openwebtext-tokenizer-gpt2\", split=\"train\"\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device = device)\n",
    "# logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- {'layer_11/width_16k/average_l0_79': 'layer_11/width_16k/average_l0_79', 'layer_1/width_16k/average_l0_102': 'layer_1/width_16k/average_l0_102', 'layer_12/width_65k/average_l0_72': 'layer_12/width_65k/average_l0_72', 'layer_12/width_16k/average_l0_82': 'layer_12/width_16k/average_l0_82', 'layer_10/width_16k/average_l0_77': 'layer_10/width_16k/average_l0_77', 'layer_0/width_16k/average_l0_106': 'layer_0/width_16k/average_l0_106', 'layer_13/width_16k/average_l0_83': 'layer_13/width_16k/average_l0_83', 'layer_13/width_65k/average_l0_74': 'layer_13/width_65k/average_l0_74', 'layer_18/width_16k/average_l0_74': 'layer_18/width_16k/average_l0_74', 'layer_18/width_65k/average_l0_117': 'layer_18/width_65k/average_l0_117', 'layer_19/width_16k/average_l0_73': 'layer_19/width_16k/average_l0_73', 'layer_19/width_65k/average_l0_115': 'layer_19/width_65k/average_l0_115', 'layer_14/width_16k/average_l0_83': 'layer_14/width_16k/average_l0_83', 'layer_2/width_16k/average_l0_141': 'layer_2/width_16k/average_l0_141', 'layer_20/width_16k/average_l0_71': 'layer_20/width_16k/average_l0_71', 'layer_20/width_65k/average_l0_114': 'layer_20/width_65k/average_l0_114', 'layer_21/width_16k/average_l0_70': 'layer_21/width_16k/average_l0_70', 'layer_21/width_65k/average_l0_112': 'layer_21/width_65k/average_l0_112', 'layer_22/width_16k/average_l0_72': 'layer_22/width_16k/average_l0_72', 'layer_22/width_65k/average_l0_117': 'layer_22/width_65k/average_l0_117', 'layer_4/width_16k/average_l0_125': 'layer_4/width_16k/average_l0_125', 'layer_23/width_65k/average_l0_124': 'layer_23/width_65k/average_l0_124', 'layer_23/width_16k/average_l0_75': 'layer_23/width_16k/average_l0_75', 'layer_5/width_16k/average_l0_68': 'layer_5/width_16k/average_l0_68', 'layer_7/width_16k/average_l0_69': 'layer_7/width_16k/average_l0_69', 'layer_3/width_16k/average_l0_59': 'layer_3/width_16k/average_l0_59', 'layer_6/width_16k/average_l0_70': 'layer_6/width_16k/average_l0_70', 'layer_9/width_16k/average_l0_73': 'layer_9/width_16k/average_l0_73'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b14ae9aaa30473a8f4423a17046f531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res\",  # <- Release name\n",
    "    sae_id=\"layer_1/width_16k/average_l0_102\",  # <- SAE id (not always a hook point!)\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
