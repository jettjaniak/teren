{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUBLAS_WORKSPACE_CONFIG=:16:8\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# needed for set_determinism\n",
    "%set_env CUBLAS_WORKSPACE_CONFIG=:16:8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/paperspace/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch plotly-express numpy sae-lens transformer-lens pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import plotly.express as px\n",
    "from datasets import Dataset, load_dataset\n",
    "from typing import cast\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def load_pretokenized_dataset(\n",
    "    path: str,\n",
    "    split: str,\n",
    ") -> Dataset:\n",
    "    dataset = load_dataset(path, split=split)\n",
    "    dataset = cast(Dataset, dataset)\n",
    "    return dataset.with_format(\"torch\")\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_device_str() -> str:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def download_sae_feature_explanations():\n",
    "    url = \"https://www.neuronpedia.org/api/explanation/export\"\n",
    "\n",
    "    # payload = {\n",
    "    #     \"modelId\": \"gpt2-small\",\n",
    "    #     \"saeId\": \"7-res-jb\",\n",
    "    # }\n",
    "    payload = {\"modelId\": \"gemma-2-2b\", \"saeId\": \"20-gemmascope-res-16k\"}\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    # convert to pandas\n",
    "    explanations_df = pd.DataFrame(response.json()[\"explanations\"])\n",
    "    # rename index to \"feature\"\n",
    "    explanations_df.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "    # explanations_df[\"feature\"] = explanations_df[\"feature\"].astype(int)\n",
    "    explanations_df[\"description\"] = explanations_df[\"description\"].apply(lambda x: x.lower())\n",
    "    return explanations_df\n",
    "\n",
    "\n",
    "def get_most_likely_tokens(logits, tokenizer, top_k=5):\n",
    "    logits = logits[0, -1, :]\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top_k_probs, top_k_indices = torch.topk(probs, k=top_k)\n",
    "\n",
    "    for prob, index in zip(top_k_probs, top_k_indices):\n",
    "        token = tokenizer.decode([index])\n",
    "        print(f\"Token: '{token}', Probability: {prob.item():.4f}\")\n",
    "\n",
    "    return tokenizer.decode([top_k_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "set_seed(5325)\n",
    "\n",
    "device = get_device_str()\n",
    "print(device)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Gemma 2 2b model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe07abf504a4dfb89e9a30b7421ae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device = device)\n",
    "logits, activations = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '!', Probability: 0.5286\n",
      "Token: ',', Probability: 0.1997\n",
      "Token: '.', Probability: 0.0678\n",
      "Token: '\n",
      "\n",
      "', Probability: 0.0312\n",
      "Token: '/', Probability: 0.0229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_likely_tokens(logits, model.tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input text\n",
    "prompt = \"1 + 1 = \"\n",
    "\n",
    "# Pass it in to the model and generate text\n",
    "logits, activations = model.run_with_cache(prompt)\n",
    "# print(model.tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: '2', Probability: 0.6732\n",
      "Token: '3', Probability: 0.1358\n",
      "Token: '1', Probability: 0.0824\n",
      "Token: '5', Probability: 0.0287\n",
      "Token: '4', Probability: 0.0249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_likely_tokens(logits, model.tokenizer, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Gemma 2 SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75af5847f4c4b6f83176305322544d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Update Link to be standard download format\n",
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\", # TODO on release s/gg-hf/google/\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class JumpReLUSAE(nn.Module):\n",
    "  def __init__(self, d_model, d_sae):\n",
    "    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "    # If you want to train your own SAEs then we recommend using blah\n",
    "    super().__init__()\n",
    "    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "    self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def encode(self, input_acts):\n",
    "    pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "    mask = (pre_acts > self.threshold)\n",
    "    acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "    return acts\n",
    "\n",
    "  def decode(self, acts):\n",
    "    return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, acts):\n",
    "    acts = self.encode(acts)\n",
    "    recon = self.decode(acts)\n",
    "    return recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JumpReLUSAE()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)\n",
    "sae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path = \"NeelNanda/pile-10k\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n",
    "\n",
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset= dataset,# type: ignore\n",
    "    tokenizer = model.tokenizer, # type: ignore\n",
    "    streaming=True,\n",
    "    max_length=128,\n",
    "    add_bos_token=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelId</th>\n",
       "      <th>layer</th>\n",
       "      <th>feature</th>\n",
       "      <th>description</th>\n",
       "      <th>scoreV1</th>\n",
       "      <th>scoreV2</th>\n",
       "      <th>autoInterpModel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>1</td>\n",
       "      <td>phrases emphasizing continuity or progression ...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>2</td>\n",
       "      <td>elements related to interviews and interrogati...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>3</td>\n",
       "      <td>instances of the verb \"to be\" in various forms</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>4</td>\n",
       "      <td>temporal references such as days, weeks, mont...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gemma-2-2b</td>\n",
       "      <td>20-gemmascope-res-16k</td>\n",
       "      <td>5</td>\n",
       "      <td>words related to trends and patterns in variou...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      modelId                  layer  feature  \\\n",
       "0  gemma-2-2b  20-gemmascope-res-16k        1   \n",
       "1  gemma-2-2b  20-gemmascope-res-16k        2   \n",
       "2  gemma-2-2b  20-gemmascope-res-16k        3   \n",
       "3  gemma-2-2b  20-gemmascope-res-16k        4   \n",
       "4  gemma-2-2b  20-gemmascope-res-16k        5   \n",
       "\n",
       "                                         description  scoreV1 scoreV2  \\\n",
       "0  phrases emphasizing continuity or progression ...        0    None   \n",
       "1  elements related to interviews and interrogati...        0    None   \n",
       "2     instances of the verb \"to be\" in various forms        0    None   \n",
       "3   temporal references such as days, weeks, mont...        0    None   \n",
       "4  words related to trends and patterns in variou...        0    None   \n",
       "\n",
       "  autoInterpModel  \n",
       "0     gpt-4o-mini  \n",
       "1     gpt-4o-mini  \n",
       "2     gpt-4o-mini  \n",
       "3     gpt-4o-mini  \n",
       "4     gpt-4o-mini  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_explanations_df = download_sae_feature_explanations()\n",
    "feature_explanations_df['feature'] = feature_explanations_df.feature.astype(int)\n",
    "feature_explanations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input text\n",
    "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "\n",
    "# Pass it in to the model and generate text\n",
    "logits, cache = model.run_with_cache(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5b89912712445e861548d8cb034335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_prompt = len(prompt)\n",
    "outputs = model.generate(prompt, max_new_tokens=50)[l_prompt + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have information that this is possible and we can figure out the technology that would need to be put in place. And here's something: It's possible to build the most advanced spaceships that ever existed and it won't even be\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k in cache.keys():\n",
    "#     if \"blocks.5.\" in k:\n",
    "#         print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"blocks.1.hook_resid_pre\"\n",
    "pos = -1\n",
    "activations = cache[layer][:, pos, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_acts = sae.encode(activations).squeeze().cpu()\n",
    "active_feature_ids = feature_acts.nonzero().squeeze().cpu()\n",
    "active_feature_acts = feature_acts[active_feature_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_feature_ids = active_feature_ids[torch.argsort(active_feature_acts, descending=True)].cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F=12054 [44.46]  phrases expressing uncertainty or opinion\n",
      "F=1508 [32.36] features related to product quality and durability\n",
      "F=6483 [24.83]  expressions of quantity or frequency\n",
      "F=13412 [24.63] checks and comparisons related to identifying the smallest or largest values\n",
      "F=14339 [15.18] mathematical notation and structures related to statistics or algorithms\n",
      "F=12842 [10.08] references to personal pronouns and possessive nouns, particularly related to individuals within a narrative context\n",
      "F=2153 [7.23] references to significant political events or figures\n",
      "F=10934 [5.12] curly braces and other related syntactical elements in structured data formats\n",
      "F=2514 [5.06] statements related to decisions and their implications\n",
      "F=6308 [5.04] references to specific legislative measures or governmental actions\n"
     ]
    }
   ],
   "source": [
    "for active_feature_id in sorted_feature_ids[:10]:\n",
    "    feature_info = dict(feature_explanations_df[feature_explanations_df.feature == active_feature_id].iloc[0])\n",
    "    feature_desc = feature_info['description']\n",
    "    feature_act = feature_acts[active_feature_id]\n",
    "    print(f\"F={active_feature_id} [{feature_act:.2f}] {feature_desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f63e71fbcd74dcdab35e15ecfa491b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    device_map='auto',\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "  _ = model.forward(inputs)\n",
    "  handle.remove()\n",
    "  return target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,  18925,    692,    614,   3326,    577,   5056,   1593,   1069,\n",
      "           2177,    476,  47420,  18216, 235336]], device='cuda:0')\n",
      "<bos>Would you be able to travel through time using a wormhole?\n",
      "\n",
      "[Answer 1]\n",
      "\n",
      "Yes, you can travel through time using a wormhole.\n",
      "\n",
      "A wormhole is a theoretical object that connects two points in space-time. It is a tunnel through space-time that allows objects to travel from\n"
     ]
    }
   ],
   "source": [
    "# The input text\n",
    "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
    "\n",
    "# Use the tokenizer to convert it to tokens. Note that this implicitly adds a special \"Beginning of Sequence\" or <bos> token to the start\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "print(inputs)\n",
    "\n",
    "# Pass it in to the model and generate text\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = gather_residual_activations(model, 20, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "recon = sae.decode(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8887, device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's just double check that the model looks sensible by checking that we explain a decent chunk of the variance:\n",
    "1 - torch.mean((recon[:, 1:] - target_act[:, 1:].to(torch.float32)) **2) / (target_act[:, 1:].to(torch.float32).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7017,   47,   65,   70,   55,   72,   65,   75,   80,   72,   68,   93,\n",
       "           86,   89]], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This probably looks OK! This SAE is supposed to have an L0 of around 70, so let's just check that too:\n",
    "(sae_acts > 1).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6631,  5482, 10376,  1670, 11023,  7562,  9407,  8399, 12935, 10004,\n",
       "         10004, 10004, 12935,  3442]], device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the highest activating features on this input text, on each token position:\n",
    "values, inds = sae_acts.max(-1)\n",
    "\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"600\"\n",
       "            src=\"https://neuronpedia.org/gemma-2-2b/20-gemmascope-res-16k/10004?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f95c8552cc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"20-gemmascope-res-16k\", feature_idx=10004)\n",
    "IFrame(html, width=1200, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenvs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
